{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3z2yeVsMvaV"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:55:17.602719Z",
          "iopub.status.busy": "2025-08-01T05:55:17.602202Z",
          "iopub.status.idle": "2025-08-01T05:55:34.930198Z",
          "shell.execute_reply": "2025-08-01T05:55:34.929379Z",
          "shell.execute_reply.started": "2025-08-01T05:55:17.602694Z"
        },
        "id": "pS-P_shPMvaX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:55:34.931976Z",
          "iopub.status.busy": "2025-08-01T05:55:34.931666Z",
          "iopub.status.idle": "2025-08-01T05:55:49.763870Z",
          "shell.execute_reply": "2025-08-01T05:55:49.762749Z",
          "shell.execute_reply.started": "2025-08-01T05:55:34.931943Z"
        },
        "id": "lBN09c1tUlSV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install latest transformers for Gemma 3N\n",
        "!pip install --no-deps --upgrade transformers # Only for Gemma 3N\n",
        "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIUrGtKNm8UA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Dataset causes too many recompiles\n",
        "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6l-xbkkm8UA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"formatted_training_dataset.jsonl\"\n",
        "VAL_DATASET_PATH = \"formatted_validation_dataset.jsonl\"\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    hf_token = os.environ[\"HF_TOKEN\"]\n",
        "else:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "    DATASET_PATH = \"/kaggle/input/example-train-data-jsonl/example_train_data.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTJQQJ00m8UA"
      },
      "outputs": [],
      "source": [
        "RUN_NAME=\"E2B-it-DCC-SFT-V2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision, Text and Audio models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c197a8c9e18f4ea4bd2abb6e65ee1a8d"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-08-01T05:55:49.765491Z",
          "iopub.status.busy": "2025-08-01T05:55:49.765189Z",
          "iopub.status.idle": "2025-08-01T05:57:39.408565Z",
          "shell.execute_reply": "2025-08-01T05:57:39.407761Z",
          "shell.execute_reply.started": "2025-08-01T05:55:49.765464Z"
        },
        "id": "-Xbb0cuLzwgf",
        "outputId": "2796d292-fe2d-483d-dce9-caff7af34e83",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            ",ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            ",==((====))==  Unsloth 2025.7.5: Fast Gemma3N patching. Transformers: 4.54.1.\n",
            ",   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
            ",O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            ",\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
            ", \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            ",Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            ",Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c197a8c9e18f4ea4bd2abb6e65ee1a8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
        "    # Pretrained models\n",
        "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other Gemma 3 quants\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3n-E2B-it\", # Or \"unsloth/gemma-3n-E4B-it\"\n",
        "    dtype = None, # None for auto detection\n",
        "    max_seq_length = 4000, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw5XPyYFajyM"
      },
      "source": [
        "# Let's finetune Gemma 3N!\n",
        "\n",
        "You can finetune the vision and text parts for now through selection - the audio part can also be finetuned - we're working to make it selectable as well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:40.422856Z",
          "iopub.status.busy": "2025-08-01T05:57:40.422566Z",
          "iopub.status.idle": "2025-08-01T05:57:45.800445Z",
          "shell.execute_reply": "2025-08-01T05:57:45.799690Z",
          "shell.execute_reply.started": "2025-08-01T05:57:40.422838Z"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "41e67287-1439-484f-c866-a6dab36a89d0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # Should leave on always!\n",
        "\n",
        "    r = 32,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 64,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Gemma-3` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<bos><start_of_turn>user\n",
        "Hello!<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Hey there!<end_of_turn>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:45.801394Z",
          "iopub.status.busy": "2025-08-01T05:57:45.801183Z",
          "iopub.status.idle": "2025-08-01T05:57:45.805812Z",
          "shell.execute_reply": "2025-08-01T05:57:45.805096Z",
          "shell.execute_reply.started": "2025-08-01T05:57:45.801378Z"
        },
        "id": "LjY75GoYUCB8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkXuGYxbJ-e"
      },
      "source": [
        "We get the first 3000 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:45.806622Z",
          "iopub.status.busy": "2025-08-01T05:57:45.806363Z",
          "iopub.status.idle": "2025-08-01T05:57:46.153729Z",
          "shell.execute_reply": "2025-08-01T05:57:46.153063Z",
          "shell.execute_reply.started": "2025-08-01T05:57:45.806578Z"
        },
        "id": "Mkq4RvEq7FQr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train[:3000]\")\n",
        "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split = \"train\")\n",
        "\n",
        "val_dataset = load_dataset(\"json\", data_files=VAL_DATASET_PATH, split = \"train\")\n",
        "val_dataset = val_dataset.shuffle(3407)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:46.154977Z",
          "iopub.status.busy": "2025-08-01T05:57:46.154716Z",
          "iopub.status.idle": "2025-08-01T05:57:46.163977Z",
          "shell.execute_reply": "2025-08-01T05:57:46.163290Z",
          "shell.execute_reply.started": "2025-08-01T05:57:46.154956Z"
        },
        "id": "reoBXmAn7HlN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_data_formats\n",
        "dataset = standardize_data_formats(dataset)\n",
        "val_dataset = standardize_data_formats(val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i5Sx9In7vHi"
      },
      "source": [
        "Let's see how row 100 looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:46.166927Z",
          "iopub.status.busy": "2025-08-01T05:57:46.166741Z",
          "iopub.status.idle": "2025-08-01T05:57:46.194812Z",
          "shell.execute_reply": "2025-08-01T05:57:46.194131Z",
          "shell.execute_reply.started": "2025-08-01T05:57:46.166913Z"
        },
        "id": "dzE1OEXi7s3P",
        "outputId": "76a882d1-3978-4163-8de2-91ed5cdd12a0",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'system',\n",
              "   'content': 'You explain programming error messages to introductory programming students.\\nKeep your response short, friendly, and without jargon.\\nProvide debugging guidance, but do not give the solution in code.\\nAddress your response to the student directly, using the pronoun \"you\".\\nFollow the provided response structure.\\n'},\n",
              "  {'role': 'user',\n",
              "   'content': 'This is my C program\\n```C\\n#include <assert.h>\\n#include <stdio.h>\\n#include <stdlib.h>\\n\\n#define MAX_LIST_LEN 100\\n\\nstruct node {\\n    int data;\\n    struct node *next;\\n};\\n\\nstruct node *delete_last(struct node *head);\\nvoid print_list(struct node *head);\\nstruct node *array_to_list(int len, int array[]);\\n\\n// DO NOT CHANGE THE MAIN FUNCTION\\nint main(void) {\\n    // Get list size\\n    int list_size;\\n    printf(\"Total numbers: \");\\n    scanf(\" %d\", &list_size);\\n\\n    // Read in numbers\\n    int list[MAX_LIST_LEN] = {0};\\n    int index_count = 0;\\n    while (index_count < list_size && scanf(\" %d\", &list[index_count])) {\\n        index_count++;\\n    }\\n\\n    // Create linked list from input numbers\\n    struct node *head = NULL;\\n    if (index_count > 0) {\\n        // List has elements\\n        head = array_to_list(list_size, list);\\n    }\\n\\n    struct node *new_head = delete_last(head);\\n    print_list(new_head);\\n\\n    return 0;\\n}\\n\\n// Delete the last element from a given linked list\\nstruct node delete_last(struct node *head) {\\n    struct node *prev = NULL;\\n    struct node *curr = NULL;\\n\\n    while (curr != NULL) {\\n        prev = curr;\\n        curr = curr->next;\\n    }\\n\\n    if (prev == NULL) {\\n        free(curr);\\n        return NULL;\\n    }\\n\\n    prev->next = curr->next;\\n    return head;\\n}\\n\\n// DO NOT CHANGE THIS FUNCTION\\n// prints the given linked list\\nvoid print_list(struct node *head) {\\n    struct node *curr = head;\\n    while (curr != NULL) {\\n        printf(\"%d -> \", curr->data);\\n        curr = curr->next;\\n    }\\n    printf(\"X\\\\n\");\\n}\\n\\n// DO NOT CHANGE THIS FUNCTION\\n// create linked list from array of strings\\nstruct node *array_to_list(int len, int array[]) {\\n    struct node *head = NULL;\\n    int i = len - 1;\\n    while (i >= 0) {\\n        struct node *n = malloc(sizeof(struct node));\\n        assert(n != NULL);\\n        n->next = head;\\n        n->data = array[i];\\n        head = n;\\n        i -= 1;\\n    }\\n    return head;\\n}\\n```\\nHelp me understand this message from the C compiler:\\n```\\nprac_q4.c:50:13: error: conflicting types for \\'delete_last\\'\\nstruct node delete_last(struct node *head) {\\n            ^\\n\\n```\\nWithout providing the solution, give me guidance on how to resolve this error, in the following structure.\\n# Response Structure\\n1. Begin with the subheading (# Error Message)\\n2. Follow with one short sentence, explaining the error message without programming jargon.\\n3. Next, write the subheading (# Potential Causes)\\n4. Follow with 1-2 short sentences, identifying and explaining potential issues in my code that may be causing this error.\\n5. Next, write the subheading (# Hints/Guidance)\\n6. Follow with 1-2 short sentences, giving debugging hints and guidance. This can include specific references to my code.\\n'},\n",
              "  {'role': 'assistant',\n",
              "   'content': '# Error Message\\nThe compiler is telling you that the way you defined the `delete_last` function does not match how you first described it.\\n\\n# Potential Causes\\nYou first said `delete_last` would return a pointer to a node, but later you defined it to return a node itself (not a pointer). This mismatch confuses the compiler.\\n\\n# Hints/Guidance\\nCheck both the function declaration at the top and the actual function definition below. Make sure the return type is exactly the same in both places, including the use of the asterisk (`*`) for pointers.'}]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xs0LXio7rfd"
      },
      "source": [
        "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:46.195877Z",
          "iopub.status.busy": "2025-08-01T05:57:46.195509Z",
          "iopub.status.idle": "2025-08-01T05:57:46.680215Z",
          "shell.execute_reply": "2025-08-01T05:57:46.679299Z",
          "shell.execute_reply.started": "2025-08-01T05:57:46.195856Z"
        },
        "id": "1ahE8Ys37JDJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"messages\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "val_dataset = val_dataset.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "Let's see how the chat template did! Notice there is no `<bos>` token as the processor tokenizer will be adding one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:46.681574Z",
          "iopub.status.busy": "2025-08-01T05:57:46.681228Z",
          "iopub.status.idle": "2025-08-01T05:57:46.686969Z",
          "shell.execute_reply": "2025-08-01T05:57:46.686074Z",
          "shell.execute_reply.started": "2025-08-01T05:57:46.681549Z"
        },
        "id": "gGFzmplrEy9I",
        "outputId": "bb37d7ea-e713-439e-f099-4d7deb9b3102",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<start_of_turn>user\\nYou explain programming error messages to introductory programming students.\\nKeep your response short, friendly, and without jargon.\\nProvide debugging guidance, but do not give the solution in code.\\nAddress your response to the student directly, using the pronoun \"you\".\\nFollow the provided response structure.\\n\\n\\nThis is my C program\\n```C\\n#include <assert.h>\\n#include <stdio.h>\\n#include <stdlib.h>\\n\\n#define MAX_LIST_LEN 100\\n\\nstruct node {\\n    int data;\\n    struct node *next;\\n};\\n\\nstruct node *delete_last(struct node *head);\\nvoid print_list(struct node *head);\\nstruct node *array_to_list(int len, int array[]);\\n\\n// DO NOT CHANGE THE MAIN FUNCTION\\nint main(void) {\\n    // Get list size\\n    int list_size;\\n    printf(\"Total numbers: \");\\n    scanf(\" %d\", &list_size);\\n\\n    // Read in numbers\\n    int list[MAX_LIST_LEN] = {0};\\n    int index_count = 0;\\n    while (index_count < list_size && scanf(\" %d\", &list[index_count])) {\\n        index_count++;\\n    }\\n\\n    // Create linked list from input numbers\\n    struct node *head = NULL;\\n    if (index_count > 0) {\\n        // List has elements\\n        head = array_to_list(list_size, list);\\n    }\\n\\n    struct node *new_head = delete_last(head);\\n    print_list(new_head);\\n\\n    return 0;\\n}\\n\\n// Delete the last element from a given linked list\\nstruct node delete_last(struct node *head) {\\n    struct node *prev = NULL;\\n    struct node *curr = NULL;\\n\\n    while (curr != NULL) {\\n        prev = curr;\\n        curr = curr->next;\\n    }\\n\\n    if (prev == NULL) {\\n        free(curr);\\n        return NULL;\\n    }\\n\\n    prev->next = curr->next;\\n    return head;\\n}\\n\\n// DO NOT CHANGE THIS FUNCTION\\n// prints the given linked list\\nvoid print_list(struct node *head) {\\n    struct node *curr = head;\\n    while (curr != NULL) {\\n        printf(\"%d -> \", curr->data);\\n        curr = curr->next;\\n    }\\n    printf(\"X\\\\n\");\\n}\\n\\n// DO NOT CHANGE THIS FUNCTION\\n// create linked list from array of strings\\nstruct node *array_to_list(int len, int array[]) {\\n    struct node *head = NULL;\\n    int i = len - 1;\\n    while (i >= 0) {\\n        struct node *n = malloc(sizeof(struct node));\\n        assert(n != NULL);\\n        n->next = head;\\n        n->data = array[i];\\n        head = n;\\n        i -= 1;\\n    }\\n    return head;\\n}\\n```\\nHelp me understand this message from the C compiler:\\n```\\nprac_q4.c:50:13: error: conflicting types for \\'delete_last\\'\\nstruct node delete_last(struct node *head) {\\n            ^\\n\\n```\\nWithout providing the solution, give me guidance on how to resolve this error, in the following structure.\\n# Response Structure\\n1. Begin with the subheading (# Error Message)\\n2. Follow with one short sentence, explaining the error message without programming jargon.\\n3. Next, write the subheading (# Potential Causes)\\n4. Follow with 1-2 short sentences, identifying and explaining potential issues in my code that may be causing this error.\\n5. Next, write the subheading (# Hints/Guidance)\\n6. Follow with 1-2 short sentences, giving debugging hints and guidance. This can include specific references to my code.<end_of_turn>\\n<start_of_turn>model\\n# Error Message\\nThe compiler is telling you that the way you defined the `delete_last` function does not match how you first described it.\\n\\n# Potential Causes\\nYou first said `delete_last` would return a pointer to a node, but later you defined it to return a node itself (not a pointer). This mismatch confuses the compiler.\\n\\n# Hints/Guidance\\nCheck both the function declaration at the top and the actual function definition below. Make sure the return type is exactly the same in both places, including the use of the asterisk (`*`) for pointers.<end_of_turn>\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[100][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ln2KOZwm8UH",
        "outputId": "770ebf40-e4a4-41f9-f87b-651a734be5ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Est. epoch Steps: 1250\n",
            ",Warmup Steps: 125\n",
            ",eval Steps: 250\n",
            ",logging every 50 steps\n"
          ]
        }
      ],
      "source": [
        "epoch_steps = 1250\n",
        "print(f\"Est. epoch Steps: {epoch_steps}\")\n",
        "warmup_steps = 125\n",
        "print(f\"Warmup Steps: {warmup_steps}\")\n",
        "\n",
        "\n",
        "# epoch_steps // (100 // EVAL_STEPS_FACTOR)\n",
        "eval_steps = 250\n",
        "logging_steps = 50\n",
        "print(f\"eval Steps: {eval_steps}\")\n",
        "checkpoint_steps = 625\n",
        "\n",
        "print(f\"logging every {logging_steps} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:46.688116Z",
          "iopub.status.busy": "2025-08-01T05:57:46.687874Z",
          "iopub.status.idle": "2025-08-01T05:57:51.716143Z",
          "shell.execute_reply": "2025-08-01T05:57:51.715320Z",
          "shell.execute_reply.started": "2025-08-01T05:57:46.688097Z"
        },
        "id": "95_Nn-89DhsL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 8, # Use GA to mimic batch size!\n",
        "        warmup_steps = warmup_steps,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 20,\n",
        "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = logging_steps,\n",
        "        optim = \"paged_adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"mlflow\",\n",
        "        run_name = RUN_NAME,\n",
        "        output_dir = \"unsloth_training_checkpoints\",\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = checkpoint_steps,\n",
        "        eval_steps = eval_steps,\n",
        "        eval_strategy = \"steps\",\n",
        "        # eval_on_start = True,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:51.717542Z",
          "iopub.status.busy": "2025-08-01T05:57:51.717255Z",
          "iopub.status.idle": "2025-08-01T05:57:52.341732Z",
          "shell.execute_reply": "2025-08-01T05:57:52.340831Z",
          "shell.execute_reply.started": "2025-08-01T05:57:51.717512Z"
        },
        "id": "juQiExuBG5Bt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again.  Notice how the sample only has a single `<bos>` as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:52.343200Z",
          "iopub.status.busy": "2025-08-01T05:57:52.342897Z",
          "iopub.status.idle": "2025-08-01T05:57:52.350769Z",
          "shell.execute_reply": "2025-08-01T05:57:52.350004Z",
          "shell.execute_reply.started": "2025-08-01T05:57:52.343171Z"
        },
        "id": "LtsMVtlkUhja",
        "outputId": "bcdfba15-5bbf-43d3-c8f0-cf2355569af2",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\nYou explain programming error messages to introductory programming students.\\nKeep your response short, friendly, and without jargon.\\nProvide debugging guidance, but do not give the solution in code.\\nAddress your response to the student directly, using the pronoun \"you\".\\nFollow the provided response structure.\\n\\n\\nThis is my C program\\n```C\\n#include <assert.h>\\n#include <stdio.h>\\n#include <stdlib.h>\\n\\n#define MAX_LIST_LEN 100\\n\\nstruct node {\\n    int data;\\n    struct node *next;\\n};\\n\\nstruct node *delete_last(struct node *head);\\nvoid print_list(struct node *head);\\nstruct node *array_to_list(int len, int array[]);\\n\\n// DO NOT CHANGE THE MAIN FUNCTION\\nint main(void) {\\n    // Get list size\\n    int list_size;\\n    printf(\"Total numbers: \");\\n    scanf(\" %d\", &list_size);\\n\\n    // Read in numbers\\n    int list[MAX_LIST_LEN] = {0};\\n    int index_count = 0;\\n    while (index_count < list_size && scanf(\" %d\", &list[index_count])) {\\n        index_count++;\\n    }\\n\\n    // Create linked list from input numbers\\n    struct node *head = NULL;\\n    if (index_count > 0) {\\n        // List has elements\\n        head = array_to_list(list_size, list);\\n    }\\n\\n    struct node *new_head = delete_last(head);\\n    print_list(new_head);\\n\\n    return 0;\\n}\\n\\n// Delete the last element from a given linked list\\nstruct node delete_last(struct node *head) {\\n    struct node *prev = NULL;\\n    struct node *curr = NULL;\\n\\n    while (curr != NULL) {\\n        prev = curr;\\n        curr = curr->next;\\n    }\\n\\n    if (prev == NULL) {\\n        free(curr);\\n        return NULL;\\n    }\\n\\n    prev->next = curr->next;\\n    return head;\\n}\\n\\n// DO NOT CHANGE THIS FUNCTION\\n// prints the given linked list\\nvoid print_list(struct node *head) {\\n    struct node *curr = head;\\n    while (curr != NULL) {\\n        printf(\"%d -> \", curr->data);\\n        curr = curr->next;\\n    }\\n    printf(\"X\\\\n\");\\n}\\n\\n// DO NOT CHANGE THIS FUNCTION\\n// create linked list from array of strings\\nstruct node *array_to_list(int len, int array[]) {\\n    struct node *head = NULL;\\n    int i = len - 1;\\n    while (i >= 0) {\\n        struct node *n = malloc(sizeof(struct node));\\n        assert(n != NULL);\\n        n->next = head;\\n        n->data = array[i];\\n        head = n;\\n        i -= 1;\\n    }\\n    return head;\\n}\\n```\\nHelp me understand this message from the C compiler:\\n```\\nprac_q4.c:50:13: error: conflicting types for \\'delete_last\\'\\nstruct node delete_last(struct node *head) {\\n            ^\\n\\n```\\nWithout providing the solution, give me guidance on how to resolve this error, in the following structure.\\n# Response Structure\\n1. Begin with the subheading (# Error Message)\\n2. Follow with one short sentence, explaining the error message without programming jargon.\\n3. Next, write the subheading (# Potential Causes)\\n4. Follow with 1-2 short sentences, identifying and explaining potential issues in my code that may be causing this error.\\n5. Next, write the subheading (# Hints/Guidance)\\n6. Follow with 1-2 short sentences, giving debugging hints and guidance. This can include specific references to my code.<end_of_turn>\\n<start_of_turn>model\\n# Error Message\\nThe compiler is telling you that the way you defined the `delete_last` function does not match how you first described it.\\n\\n# Potential Causes\\nYou first said `delete_last` would return a pointer to a node, but later you defined it to return a node itself (not a pointer). This mismatch confuses the compiler.\\n\\n# Hints/Guidance\\nCheck both the function declaration at the top and the actual function definition below. Make sure the return type is exactly the same in both places, including the use of the asterisk (`*`) for pointers.<end_of_turn>\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kyjy__m9KY3"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:52.352179Z",
          "iopub.status.busy": "2025-08-01T05:57:52.351865Z",
          "iopub.status.idle": "2025-08-01T05:57:52.380837Z",
          "shell.execute_reply": "2025-08-01T05:57:52.380074Z",
          "shell.execute_reply.started": "2025-08-01T05:57:52.352159Z"
        },
        "id": "_rD6fl8EUxnG",
        "outputId": "a4235fa9-2fbf-450b-c93d-ec6cf374504c",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      # Error Message\\nThe compiler is telling you that the way you defined the `delete_last` function does not match how you first described it.\\n\\n# Potential Causes\\nYou first said `delete_last` would return a pointer to a node, but later you defined it to return a node itself (not a pointer). This mismatch confuses the compiler.\\n\\n# Hints/Guidance\\nCheck both the function declaration at the top and the actual function definition below. Make sure the return type is exactly the same in both places, including the use of the asterisk (`*`) for pointers.<end_of_turn>\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "execution_failed": "2025-08-01T06:22:31.016Z",
          "iopub.execute_input": "2025-08-01T05:57:52.381768Z",
          "iopub.status.busy": "2025-08-01T05:57:52.381541Z",
          "iopub.status.idle": "2025-08-01T05:57:53.042031Z"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "99592e08-f375-42b0-ecb0-0acf240a2720",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.254 GB.\n",
            ",7.766 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "# Let's train the model!\n",
        "\n",
        "To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T05:57:53.043499Z",
          "iopub.status.busy": "2025-08-01T05:57:53.042908Z",
          "iopub.status.idle": "2025-08-01T06:03:18.089796Z",
          "shell.execute_reply": "2025-08-01T06:03:18.089073Z",
          "shell.execute_reply.started": "2025-08-01T05:57:53.043473Z"
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "8c368cf7-5f59-4de1-8a42-b5eb30c14ee4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            ",   \\\\   /|    Num examples = 40,000 | Num Epochs = 1 | Total steps = 1,250\n",
            ",O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
            ",\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
            ", \"-____-\"     Trainable parameters = 42,270,720 of 5,481,708,992 (0.77% trained)\n",
            ",`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 6:57:32, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>7.237900</td>\n",
              "      <td>1.157944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.230800</td>\n",
              "      <td>1.110996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>5.805000</td>\n",
              "      <td>1.113060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.527100</td>\n",
              "      <td>1.109712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>5.426800</td>\n",
              "      <td>1.118261</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but Gemma3nForConditionalGeneration does not accept `num_items_in_batch`.\n",
            ",Using gradient accumulation will be very slightly less accurate.\n",
            ",Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2025-08-01T06:03:18.091569Z",
          "iopub.status.busy": "2025-08-01T06:03:18.090656Z",
          "iopub.status.idle": "2025-08-01T06:03:19.104664Z",
          "shell.execute_reply": "2025-08-01T06:03:19.103853Z",
          "shell.execute_reply.started": "2025-08-01T06:03:18.091548Z"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "14236050-bc1c-4ca8-a9ab-1f66635839dc",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25078.5991 seconds used for training.\n",
            ",417.98 minutes used for training.\n",
            ",Peak reserved memory = 64.391 GB.\n",
            ",Peak reserved memory for training = 56.625 GB.\n",
            ",Peak reserved memory % of max memory = 81.246 %.\n",
            ",Peak reserved memory for training % of max memory = 71.447 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T06:03:19.105739Z",
          "iopub.status.busy": "2025-08-01T06:03:19.105464Z",
          "iopub.status.idle": "2025-08-01T06:04:10.388800Z",
          "shell.execute_reply": "2025-08-01T06:04:10.388014Z",
          "shell.execute_reply.started": "2025-08-01T06:03:19.105721Z"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "758f6ed8-6c93-4bd5-aa3b-16b6546c6e1e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<bos><start_of_turn>user\\nYou explain programming error messages to introductory programming students.\\nKeep your response short, friendly, and without jargon.\\nProvide debugging guidance, but do not give the solution in code.\\nAddress your response to the student directly, using the pronoun \"you\".\\nFollow the provided response structure.\\nThis is my C program\\n```C\\n#include <stdio.h>\\n\\nint main(void) {\\n\\tchar tempChar;\\n\\n\\tscanf(\"%c\", &tempChar);\\n\\n\\twhile (tempChar != \\'\\\\0\\') {\\n\\t\\tif (tempChar != \\'a\\' && tempChar != \\'e\\' && tempChar != \\'i\\' && tempChar != \\'o\\' && tempChar != \\'u\\') {\\n\\t\\t\\tprintf(\"%c\", tempChar);\\n\\t\\t}\\n\\n\\t\\tscanf(\"%c\", &tempChar);\\n\\t}\\n\\n}\\n```\\nHelp me understand this error:\\n```\\nExecution was interruptedExecution stopped in main() in no_vowels.c at line 13:\\n\\n\\twhile (tempChar != \\'\\\\0\\') {\\n\\t\\tif (tempChar != \\'a\\' && tempChar != \\'e\\' && tempChar != \\'i\\' && tempChar != \\'o\\' && tempChar != \\'u\\') {\\n\\t\\t\\tprintf(\"%c\", tempChar);\\n\\t\\t}\\n\\n-->\\t\\tscanf(\"%c\", &tempChar);\\n\\t}\\n\\n}\\ntempChar = 10 = \\'\\\\n\\'\\n\\n```\\nIt was given this input\\n```\\n\"Peter piper picked a pickled pepper\".\\n```\\nWithout providing the solution, give me guidance on how to resolve this error, in the following structure.\\n# Response Structure\\n1. Begin with the subheading (# Error Message)\\n2. Follow with one short sentence, explaining the error message without programming jargon.\\n3. Next, write the subheading (# Potential Causes)\\n4. Follow with 1-2 short sentences, identifying and explaining potential issues in my code that may be causing this error.\\n5. Next, write the subheading (# Hints/Guidance)\\n6. Follow with 1-2 short sentences, giving debugging hints and guidance. This can include specific references to my code.<end_of_turn>\\n<start_of_turn>model\\n# Error Message\\nYour program is getting stuck in a loop and not processing all the characters.\\n\\n# Potential Causes\\nThe `scanf(\"%c\", &tempChar);` inside your `while` loop is never set to `\\'\\\\0\\'` (the null character), so the loop keeps running forever. You need to find the end of the input to stop the loop.\\n\\n# Hints/Guidance\\nLook at how you can check for the null terminator in a string buffer that `scanf(\"%c\", &tempChar);` is reading from. Once you find it, you can exit the loop gracefully.<end_of_turn>']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\" : \"text\",\n",
        "        \"text\" : \"You explain programming error messages to introductory programming students.\\nKeep your response short, friendly, and without jargon.\\nProvide debugging guidance, but do not give the solution in code.\\nAddress your response to the student directly, using the pronoun \\\"you\\\".\\nFollow the provided response structure.\\nThis is my C program\\n```C\\n#include <stdio.h>\\n\\nint main(void) {\\n\\tchar tempChar;\\n\\n\\tscanf(\\\"%c\\\", &tempChar);\\n\\n\\twhile (tempChar != '\\\\0') {\\n\\t\\tif (tempChar != 'a' && tempChar != 'e' && tempChar != 'i' && tempChar != 'o' && tempChar != 'u') {\\n\\t\\t\\tprintf(\\\"%c\\\", tempChar);\\n\\t\\t}\\n\\n\\t\\tscanf(\\\"%c\\\", &tempChar);\\n\\t}\\n\\n}\\n```\\nHelp me understand this error:\\n```\\nExecution was interruptedExecution stopped in main() in no_vowels.c at line 13:\\n\\n\\twhile (tempChar != '\\\\0') {\\n\\t\\tif (tempChar != 'a' && tempChar != 'e' && tempChar != 'i' && tempChar != 'o' && tempChar != 'u') {\\n\\t\\t\\tprintf(\\\"%c\\\", tempChar);\\n\\t\\t}\\n\\n-->\\t\\tscanf(\\\"%c\\\", &tempChar);\\n\\t}\\n\\n}\\ntempChar = 10 = '\\\\n'\\n\\n```\\nIt was given this input\\n```\\n\\\"Peter piper picked a pickled pepper\\\".\\n```\\nWithout providing the solution, give me guidance on how to resolve this error, in the following structure.\\n# Response Structure\\n1. Begin with the subheading (# Error Message)\\n2. Follow with one short sentence, explaining the error message without programming jargon.\\n3. Next, write the subheading (# Potential Causes)\\n4. Follow with 1-2 short sentences, identifying and explaining potential issues in my code that may be causing this error.\\n5. Next, write the subheading (# Hints/Guidance)\\n6. Follow with 1-2 short sentences, giving debugging hints and guidance. This can include specific references to my code.\\n\",\n",
        "    }]\n",
        "}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        "    tokenize = True,\n",
        "    return_dict = True,\n",
        ").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 150, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T06:04:10.389758Z",
          "iopub.status.busy": "2025-08-01T06:04:10.389552Z",
          "iopub.status.idle": "2025-08-01T06:04:10.393334Z",
          "shell.execute_reply": "2025-08-01T06:04:10.392557Z",
          "shell.execute_reply.started": "2025-08-01T06:04:10.389744Z"
        },
        "id": "e2pEuRb1r2Vg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# messages = [{\n",
        "#     \"role\": \"user\",\n",
        "#     \"content\": [{\"type\" : \"text\", \"text\" : \"Why is the sky blue?\",}]\n",
        "# }]\n",
        "# inputs = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     add_generation_prompt = True, # Must add for generation\n",
        "#     return_tensors = \"pt\",\n",
        "#     tokenize = True,\n",
        "#     return_dict = True,\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# _ = model.generate(\n",
        "#     **inputs,\n",
        "#     max_new_tokens = 64, # Increase for longer outputs!\n",
        "#     # Recommended Gemma-3 settings!\n",
        "#     temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "#     streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMZFaaL7m8UN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "383bcbac589c44738f5a4bc18b1f6838",
            "ae9f8f988499417fb590bda6ef5801f4",
            "f9a858a5c53e4dff8accbee50342ec34",
            "d9f36dbc60dd44df8d6bbc163ef0e43b",
            "08dc3db3d531423a8dab19b602078e84",
            "ce75d18e270141f386cd763759a44951"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-08-01T06:04:10.394494Z",
          "iopub.status.busy": "2025-08-01T06:04:10.394214Z",
          "iopub.status.idle": "2025-08-01T06:04:15.057997Z",
          "shell.execute_reply": "2025-08-01T06:04:15.057226Z",
          "shell.execute_reply.started": "2025-08-01T06:04:10.394473Z"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "f8bc2909-78be-4999-d1e8-b9fc72d2c377",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "383bcbac589c44738f5a4bc18b1f6838",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae9f8f988499417fb590bda6ef5801f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9a858a5c53e4dff8accbee50342ec34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/169M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/z5258621/gemma-3n-E2B-it-DCC-SFT-V2-lora\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9f36dbc60dd44df8d6bbc163ef0e43b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08dc3db3d531423a8dab19b602078e84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce75d18e270141f386cd763759a44951",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.save_pretrained(\"gemma-3n-E2B-it-DCC-SFT-V2\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3n-E2B-it-DCC-SFT-V2\")\n",
        "model.push_to_hub(\"repo/gemma-3n-E2B-it-DCC-SFT-V2-lora\", token = hf_token, private=True) # Online saving\n",
        "tokenizer.push_to_hub(\"repo/gemma-3n-E2B-it-DCC-SFT-V2-lora\", token = hf_token, private=True) # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T06:04:15.059211Z",
          "iopub.status.busy": "2025-08-01T06:04:15.058901Z",
          "iopub.status.idle": "2025-08-01T06:04:50.050032Z",
          "shell.execute_reply": "2025-08-01T06:04:50.049486Z",
          "shell.execute_reply.started": "2025-08-01T06:04:15.059183Z"
        },
        "id": "MKX_XKs_BNZR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastModel\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3N?\",}]\n",
        "}]\n",
        "# inputs = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     add_generation_prompt = True, # Must add for generation\n",
        "#     return_tensors = \"pt\",\n",
        "#     tokenize = True,\n",
        "#     return_dict = True,\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# _ = model.generate(\n",
        "#     **inputs,\n",
        "#     max_new_tokens = 128, # Increase for longer outputs!\n",
        "#     # Recommended Gemma-3 settings!\n",
        "#     temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "#     streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3n-E2B-it-DCC-SFT-V2`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fc283498c9f246aaa4a7f2982aeb6b91",
            "2bbeea5a6c0c4b51bf11de1ac7152436",
            "60c49eb2378a400598f01cd660e1a19a",
            "c13581c4a5be4d3cb709e68893f6adb8"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-08-01T06:04:50.050979Z",
          "iopub.status.busy": "2025-08-01T06:04:50.050741Z",
          "iopub.status.idle": "2025-08-01T06:07:08.324989Z",
          "shell.execute_reply": "2025-08-01T06:07:08.324122Z",
          "shell.execute_reply.started": "2025-08-01T06:04:50.050951Z"
        },
        "id": "iHjt_SMYsd3P",
        "outputId": "c168c8e0-37af-413d-9ca6-81db61c27732",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found HuggingFace hub cache directory: /home/jovyan/.cache/huggingface/hub\n",
            ",Checking cache directory for required files...\n",
            ",Cache check failed: model-00001-of-00003.safetensors not found in local cache.\n",
            ",Not all required files found in cache. Will proceed with downloading.\n",
            ",Downloading safetensors index for unsloth/gemma-3n-e2b-it...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc283498c9f246aaa4a7f2982aeb6b91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bbeea5a6c0c4b51bf11de1ac7152436",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:26<00:52, 26.34s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60c49eb2378a400598f01cd660e1a19a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:26<00:46, 46.41s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c13581c4a5be4d3cb709e68893f6adb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:53<00:00, 37.79s/it]\n"
          ]
        }
      ],
      "source": [
        "if True: # Change to True to save finetune!\n",
        "    model.save_pretrained_merged(\"gemma-3n-E2B-it-DCC-SFT-V2\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6O48DbNIAr0"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1d5cc70cb5d34437a440d5f8a5508645",
            "4c0e8335917b4eafbfaf5f1b81a32d21",
            "174146e5422d424a9f4264883de3bf0d",
            "db322fecaa8c492e8180e62cdd3cc707",
            "c253a2a81c334ca4b7cebbf88c0d7c29",
            "4da8379d963348ab9b05a8423bc76e41",
            "aae018ba430147eda366f06b5cbb1725",
            "fcada5807968448aa553ed057f6360a6",
            "55dcf9bb557e470ebb61895024ec8db6",
            "67413ac3ae574d96b9d1c895bcca53c4",
            "03942dc989fd4693acd0df778b02cdd3",
            "2bb09cdf09ac4250b29bd8e46f59fa11",
            "188c5b1d3075488e9d5df916e2c0af0c"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-08-01T06:07:11.538401Z",
          "iopub.status.busy": "2025-08-01T06:07:11.538138Z",
          "iopub.status.idle": "2025-08-01T06:11:51.231466Z",
          "shell.execute_reply": "2025-08-01T06:11:51.230640Z",
          "shell.execute_reply.started": "2025-08-01T06:07:11.538381Z"
        },
        "id": "ZV-CiKPrIFG0",
        "outputId": "557ac36c-c997-4af5-a711-0a4f27a78ca7",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d5cc70cb5d34437a440d5f8a5508645",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c0e8335917b4eafbfaf5f1b81a32d21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "174146e5422d424a9f4264883de3bf0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found HuggingFace hub cache directory: /home/jovyan/.cache/huggingface/hub\n",
            ",Checking cache directory for required files...\n",
            ",Cache check failed: model-00001-of-00003.safetensors not found in local cache.\n",
            ",Not all required files found in cache. Will proceed with downloading.\n",
            ",Downloading safetensors index for unsloth/gemma-3n-e2b-it...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db322fecaa8c492e8180e62cdd3cc707",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c253a2a81c334ca4b7cebbf88c0d7c29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4da8379d963348ab9b05a8423bc76e41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aae018ba430147eda366f06b5cbb1725",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:40<01:21, 40.72s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcada5807968448aa553ed057f6360a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55dcf9bb557e470ebb61895024ec8db6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67413ac3ae574d96b9d1c895bcca53c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:14<01:12, 72.13s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03942dc989fd4693acd0df778b02cdd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bb09cdf09ac4250b29bd8e46f59fa11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "188c5b1d3075488e9d5df916e2c0af0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:13<00:00, 64.54s/it]\n"
          ]
        }
      ],
      "source": [
        "if True: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"repo/gemma-3n-E2B-it-DCC-SFT-V2\", tokenizer,\n",
        "        token = hf_token,\n",
        "        private=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T06:14:51.904408Z",
          "iopub.status.busy": "2025-08-01T06:14:51.903977Z",
          "iopub.status.idle": "2025-08-01T06:21:32.924853Z",
          "shell.execute_reply": "2025-08-01T06:21:32.920502Z",
          "shell.execute_reply.started": "2025-08-01T06:14:51.904378Z"
        },
        "id": "FqfebeAdT073",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# if True: # Change to True to save to GGUF\n",
        "#     model.save_pretrained_gguf(\n",
        "#         \"gemma-3n-E2B-it-DCC-SFT-V2\",\n",
        "#         quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "#     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q974YEVPI7JS"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1316fce353a64a6fa25f58276a797b6c"
          ]
        },
        "execution": {
          "iopub.status.busy": "2025-08-01T06:21:32.925304Z",
          "iopub.status.idle": "2025-08-01T06:21:32.925527Z",
          "shell.execute_reply": "2025-08-01T06:21:32.925431Z",
          "shell.execute_reply.started": "2025-08-01T06:21:32.925421Z"
        },
        "id": "ZgcJIhJ0I_es",
        "outputId": "b23d090f-11c2-4da8-af86-084defa6e314",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3n-E2B-it-DCC-SFT-V2\n",
            ",Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            ",Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            ",Unsloth GGUF:hf-to-gguf:Exporting model...\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
            ",Unsloth GGUF:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> Q8_0, shape = {2048, 2048, 3}\n",
            ",Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {2048, 262144}\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_token_embd.weight,       torch.bfloat16 --> Q8_0, shape = {7680, 262144}\n",
            ",Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2048}\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_model_proj.weight,       torch.bfloat16 --> Q8_0, shape = {2048, 7680}\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_proj_norm.weight,        torch.bfloat16 --> F32, shape = {256}\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:Set meta model\n",
            ",Unsloth GGUF:hf-to-gguf:Set model parameters\n",
            ",Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
            ",Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_sep_token to False\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
            ",Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            ",{%- if messages[0]['role'] == 'system' -%}\n",
            ",    {%- if messages[0]['content'] is string -%}\n",
            ",        {%- set first_user_prefix = messages[0]['content'] + '\n",
            ",\n",
            ",' -%}\n",
            ",    {%- else -%}\n",
            ",        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            ",..... Chat template truncated .....\n",
            ",\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1316fce353a64a6fa25f58276a797b6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
            ",Unsloth: Converted to gemma-3n-E2B-it-DCC-SFT-V2.Q8_0.gguf with size = 4.7G\n",
            ",Unsloth: Successfully saved GGUF to:\n",
            ",gemma-3n-E2B-it-DCC-SFT-V2.Q8_0.gguf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "if True: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3n-E2B-it-DCC-SFT-V2\",\n",
        "        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"repo/gemma-3n-E2B-it-DCC-SFT-V2-gguf\",\n",
        "        token = hf_token,\n",
        "        # private=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fd1aea341e5e46fdbaaca1ebb2c6f146"
          ]
        },
        "id": "pyny6R98m8UQ",
        "outputId": "f5543ab3-e989-47a0-e409-c81f57ba3a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3n-E2B-it-DCC-SFT-V2\n",
            ",Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            ",Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            ",Unsloth GGUF:hf-to-gguf:Exporting model...\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> BF16, shape = {2048, 2048, 3}\n",
            ",Unsloth GGUF:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> BF16, shape = {2048, 2048, 3}\n",
            ",Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> BF16, shape = {2048, 262144}\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_token_embd.weight,       torch.bfloat16 --> BF16, shape = {7680, 262144}\n",
            ",Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2048}\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_model_proj.weight,       torch.bfloat16 --> BF16, shape = {2048, 7680}\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_proj_norm.weight,        torch.bfloat16 --> F32, shape = {256}\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:Set meta model\n",
            ",Unsloth GGUF:hf-to-gguf:Set model parameters\n",
            ",Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
            ",Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_sep_token to False\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
            ",Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            ",{%- if messages[0]['role'] == 'system' -%}\n",
            ",    {%- if messages[0]['content'] is string -%}\n",
            ",        {%- set first_user_prefix = messages[0]['content'] + '\n",
            ",\n",
            ",' -%}\n",
            ",    {%- else -%}\n",
            ",        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            ",..... Chat template truncated .....\n",
            ",\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd1aea341e5e46fdbaaca1ebb2c6f146",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
            ",Unsloth: Converted to gemma-3n-E2B-it-DCC-SFT-V2.BF16.gguf with size = 8.9G\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Successfully saved GGUF to:\n",
            ",gemma-3n-E2B-it-DCC-SFT-V2.BF16.gguf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "if True: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3n-E2B-it-DCC-SFT-V2\",\n",
        "        quantization_type = \"BF16\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"repo/gemma-3n-E2B-it-DCC-SFT-V2-gguf\",\n",
        "        token = hf_token,\n",
        "        # private=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5f8a72778dc64a2681790634ae23fc62"
          ]
        },
        "id": "ZKcCJRl1m8UR",
        "outputId": "a67d0717-739a-44c0-af12-18b043c11f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3n-E2B-it-DCC-SFT-V2\n",
            ",Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            ",Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            ",Unsloth GGUF:hf-to-gguf:Exporting model...\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            ",Unsloth GGUF:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            ",Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> F16, shape = {2048, 262144}\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_token_embd.weight,       torch.bfloat16 --> F16, shape = {7680, 262144}\n",
            ",Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2048}\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_model_proj.weight,       torch.bfloat16 --> F16, shape = {2048, 7680}\n",
            ",Unsloth GGUF:hf-to-gguf:per_layer_proj_norm.weight,        torch.bfloat16 --> F32, shape = {256}\n",
            ",Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
            ",Unsloth GGUF:hf-to-gguf:Set meta model\n",
            ",Unsloth GGUF:hf-to-gguf:Set model parameters\n",
            ",Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
            ",Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
            ",Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_sep_token to False\n",
            ",Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
            ",Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            ",{%- if messages[0]['role'] == 'system' -%}\n",
            ",    {%- if messages[0]['content'] is string -%}\n",
            ",        {%- set first_user_prefix = messages[0]['content'] + '\n",
            ",\n",
            ",' -%}\n",
            ",    {%- else -%}\n",
            ",        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            ",..... Chat template truncated .....\n",
            ",\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f8a72778dc64a2681790634ae23fc62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
            ",Unsloth: Converted to gemma-3n-E2B-it-DCC-SFT-V2.F16.gguf with size = 8.9G\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Successfully saved GGUF to:\n",
            ",gemma-3n-E2B-it-DCC-SFT-V2.F16.gguf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "if True: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3n-E2B-it-DCC-SFT-V2\",\n",
        "        quantization_type = \"F16\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"repo/gemma-3n-E2B-it-DCC-SFT-V2-gguf\",\n",
        "        token = hf_token,\n",
        "        # private=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-01T06:22:04.345159Z",
          "iopub.status.busy": "2025-08-01T06:22:04.344550Z",
          "iopub.status.idle": "2025-08-01T06:22:04.349397Z",
          "shell.execute_reply": "2025-08-01T06:22:04.348557Z",
          "shell.execute_reply.started": "2025-08-01T06:22:04.345136Z"
        },
        "id": "4BxIcYYBm8UR",
        "outputId": "8f11d25a-d1a6-4a89-f9e5-474245d0c59f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ",FROM {__FILE_LOCATION__}\n",
            ",TEMPLATE \"\"\"{{- range $i, $_ := .Messages }}\n",
            ",{{- $last := eq (len (slice $.Messages $i)) 1 }}\n",
            ",{{- if or (eq .Role \"user\") (eq .Role \"system\") }}<start_of_turn>user\n",
            ",{{ .Content }}<end_of_turn>\n",
            ",{{ if $last }}<start_of_turn>model\n",
            ",{{ end }}\n",
            ",{{- else if eq .Role \"assistant\" }}<start_of_turn>model\n",
            ",{{ .Content }}{{ if not $last }}<end_of_turn>\n",
            ",{{ end }}\n",
            ",{{- end }}\n",
            ",{{- end }}\"\"\"\n",
            ",PARAMETER stop \"<end_of_turn>\"\n",
            ",PARAMETER stop \"<eos>\"\n",
            ",PARAMETER temperature 0.1\n",
            ",PARAMETER min_p 0.0\n",
            ",PARAMETER top_k 64\n",
            ",PARAMETER top_p 0.95\n",
            ",PARAMETER num_predict 32768\n",
            ",\n"
          ]
        }
      ],
      "source": [
        "# May need to save this to a file.\n",
        "print(tokenizer._ollama_modelfile)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 12693789,
          "sourceId": 105267,
          "sourceType": "competition"
        },
        {
          "datasetId": 7985880,
          "sourceId": 12637801,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "dcc_sft_notebooks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
